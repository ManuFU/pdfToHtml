            <html>
                <body>
                    <pre>See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/259510668
Enabling cost-aware and adaptive elasticity of multi-tier cloud applications
Article  in  Future Generation Computer Systems · March 2014
DOI: 10.1016/j.future.2012.05.018
CITATIONS
215
READS
2,357
5 authors, including:
Some of the authors of this publication are also working on these related projects:
IMI eTRIKS project View project
COVID-19 View project
Rui Han
Chinese Academy of Sciences
37 PUBLICATIONS   1,014 CITATIONS   
SEE PROFILE
Moustafa Ghanem
Middlesex University, UK
90 PUBLICATIONS   2,463 CITATIONS   
SEE PROFILE
Li Guo
Manchester Metropolitan University
64 PUBLICATIONS   1,200 CITATIONS   
SEE PROFILE
Yike Guo
Imperial College London
372 PUBLICATIONS   9,285 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Rui Han on 22 August 2018.
The user has requested enhancement of the downloaded file.
Future Generation Computer Systems ( ) –
Contents lists available at SciVerse ScienceDirect
Future Generation Computer Systems
journal homepage: www.elsevier.com/locate/fgcs
Enabling cost-aware and adaptive elasticity of multi-tier cloud applications
Rui Han, Moustafa M. Ghanem, Li Guo, Yike Guo ∗, Michelle Osmond
Department of Computing, Imperial College London, London SW7 2AZ, UK
a r t i c l e i n f o
Article history:
Received 30 October 2011
Received in revised form
30 March 2012
Accepted 17 May 2012
Available online xxxx
Keywords:
Cloud computing
Elasticity
Multi-tier applications
Cost-aware criteria
Adaptive scaling algorithm
a b s t r a c t
Elasticity (on-demand scaling) of applications is one of the most important features of cloud computing.
This elasticity is the ability to adaptively scale resources up and down in order tomeet varying application
demands. To date, most existing scaling techniques can maintain applications’ Quality of Service (QoS)
but do not adequately address issues relating to minimizing the costs of using the service. In this paper,
we propose an elastic scaling approach that makes use of cost-aware criteria to detect and analyse
the bottlenecks within multi-tier cloud-based applications. We present an adaptive scaling algorithm
that reduces the costs incurred by users of cloud infrastructure services, allowing them to scale their
applications only at bottleneck tiers, and present the design of an intelligent platform that automates the
scaling process. Our approach is generic for awide class ofmulti-tier applications, andwe demonstrate its
effectiveness against other approaches by studying the behaviour of an example e-commerce application
using a standard workload benchmark.
© 2012 Elsevier B.V. All rights reserved.
1. Introduction
Cloud computing has received wide attention over the past
few years. New services offered by cloud IaaS (Infrastructure-
as-a-Service) providers, such as Amazon Web Services (WS) [1],
GoGrid [2] and IBM [3], are generating a huge demand from ap-
plication owners. The pay-as-you gomodel used by such providers
is appealing to most application owners. It removes the costs of
buying, installing and maintaining a dedicated infrastructure for
running an application. Moreover, most IaaS providers allow the
application owners to scale up and down the resources used based
on the computational demands of their applications, thus letting
them pay only for the amount of resources they use. This model is
appealing for deploying applications that provide services for third
parties, e.g. traditional e-commerce sites, financial services appli-
cations, online healthcare applications, gaming applications, me-
dia servers and bioinformatics applications. If the workload of a
service increases (e.g. more end users start submitting requests at
the same time), the application owner can ideally scale up the re-
sources used to maintain the Quality of Service (QoS) of their ser-
vice.When theworkload eases down, they can then scale down the
resources used. Within this context, elasticity (on-demand scal-
ing), also known as redeploying or dynamic provisioning, of ap-
plications has become one of the most important features of a
∗ Corresponding author. Tel.: +44 07869562039.
E-mail addresses: r.han10@imperial.ac.uk (R. Han), mmg@imperial.ac.uk
(M.M. Ghanem), liguo@imperial.ac.uk (L. Guo), y.guo@imperial.ac.uk (Y. Guo),
mo197@imperial.ac.uk (M. Osmond).
cloud computing platform. This elasticity enables real-time ac-
quisition/release of computing resources to scale the applications
themselves up and down in order to meet their run-time require-
ments, while letting application owners pay only for the resources
used.
Our motivation in this paper is investigating the development
of new methods that assist the owners of applications deployed
on IaaS clouds in managing the costs of their own applications
while still maintaining the Quality of Service (QoS) they provide
to their end users. Addressing this issue effectively requires taking
a closer look at the structure of most common services and
applications deployed on IaaS clouds to provide services to other
parties. Such applications are typically implemented as multi-tier
applications running on distributed software platforms. Taking the
example of an e-commerce website, there are at least three tiers:
a frontend web server for handling HTTP requests; a middle-tier
application server for implementing business logic; and a backend
database with data store and processing. Each of the tiers can be
implemented using one or more servers. Depending on different
types of incoming requests, servers at each tier can be stressed by
heavyworkloads, or can become idle due to light workloads.When
scaling up and down an application, it is thus crucial to discover the
real bottlenecks that may be caused at any, or all, of the servers.
Although some of the existing scaling techniques [1,4–16]
address the question of how to maintain an applications’ Quality
of Service (QoS), they rarely consider the equally important aspect
of cloud computing—the cost of using the resources themselves.
Applications deployed in a cloud environment require both good
performance and cost-efficient resource usage.
In this paper, we propose a scaling approach that is both cost-
aware and workload-adaptive, allowing application owners to
0167-739X/$ – see front matter© 2012 Elsevier B.V. All rights reserved.
doi:10.1016/j.future.2012.05.018
2 R. Han et al. / Future Generation Computer Systems ( ) –
Fig. 1. Multi-tier infrastructure of an e-commerce website and three types of workloads.
perform more efficient cloud elasticity management. The paper
features four key elements:
• Cost-aware criteria: a flexible analytical model is developed
to capture the behaviour of multi-tier applications. Cost-aware
criteria are introduced tomeasure the effect of cost of resources
on every unit of response time.
• Workload-adaptive scaling: using the above criteria, a Cost-
Aware Scaling (CAS) algorithm is designed to handle changing
workloads of multi-tier applications by adaptively scaling up
and down bottlenecked tiers within applications.
• Automation of application scaling: a standard and extensible
specification is introduced to describe the properties of the
servers, including their VM configuration, IaaS user settings,
linking relationships and other constraints. Based on this
specification, the best cost-aware scaling approach required for
an application can be automatically computed and executed.
• Implementation and experimental evaluation: an intelligent
platform based on the CAS algorithm is implemented to
automate the scaling process of cloud applications on the
IC-Cloud infrastructure [17]. The proposed cost-aware ap-
proach is tested using an industry standard benchmark [18] and
the test results show: (1) the CAS algorithm responds to chang-
ing workloads effectively by scaling applications up and down
appropriately to meet their QoS requirements; (2) deployment
costs are reduced compared to other scaling techniques.
The remainder of this paper is organised as follows: Section 2 il-
lustrates the need for this new approach to elasticity by describing
some current examples and challenges; Section 3 discusses related
work; Section 4 provides a more detailed overview of the proper-
ties of typicalmulti-tier applications anddescribes the architecture
of the Imperial Smart Scaling engine (iSSe) implemented to support
our approach; Section 5 explains the proposed CAS algorithm and
its details; Section 6 reports the experimental evaluation of the al-
gorithm’s effectiveness; Sections 7 and 8 present discussion of the
approach and summarise directions for future work.
2. Motivation
This section illustrates two challenges that need to be addressed
in order to achieve elastic scaling in a large class of multi-tier
applications deployed on IaaS clouds. Without loss of generality,
we use a simple example based on an e-commerce website
to capture the typical behaviour of such applications. Also for
simplicity, we focus only on applications that are deployed on
the resources of single IaaS cloud provider. As discussed in the
introduction, the workload for such applications depends on the
number of end users submitting requests at the same time. The
workloadmay be composed of different types of requests that need
to be handled by different parts of the application. For example
some end users may be browsing the web site itself, while others
may be querying the product catalogue or making a payment
transaction. To highlight the key challenges, consider the typical
infrastructure for amulti-tier e-commerce application as shown in
Fig. 1(a). This application is composed of five tiers of components
(servers): the HAProxy and Amoeba load-balancing servers, the
Apache HTTP server, the Tomcat application server and the MySQL
database server. These servers work together to handle end users’
requests. Depending on the application workload, servers at each
tier can be stressed at different times and the application owner
would need to scale up or down the appropriate resources to
maintain the QoS of their application.
Challenge 1: Cost-aware scaling. In a highly scalable cloud en-
vironment where computing resources are consumed as a utility
such as water and electricity [19], application owners would ex-
pect to spend the least cost for the desired application perfor-
mance. To this aim, the elastic scalingmust take cost-aware criteria
into consideration and use them to guide application scaling. Take
Fig. 1(a)’s application for example, these criteria should be aware
of both the cost of adding a server (e.g., an Apache or aMySQL) and
the performance effect brought by this scaling up (e.g., reducing
response time).
When the application is initially deployed (Fig. 1(a)), five
servers of this application are hosted across different VMs to
support a small number of customers.When the demand increases,
the application should be scaled up. An interesting point here is
that this scaling process is greatly influenced by the behaviour
(i.e., the type of workload) of the application itself. We examine
three typical types of workloads, where each workload places
varying demands on different tiers of the application. In the
primarily ‘‘browsing’’ workload (Fig. 1(b)), end users mainly
browse webpages and preview products. This workload mainly
stresses the service tier including the Apache and Tomcat servers,
so their resources are saturated and the number of these servers
needs to be increased. By contrast, the primarily ‘‘ordering’’
workload mainly stresses the storage tier including the MySQL
database and so the number of these database servers needs
increasing (Fig. 1(c)). Finally, the typical ‘‘shopping’’ workload
simultaneously stresses the service and storage tiers and so the
number of servers in both two tiers is increased (Fig. 1(d)).
Challenge 2: Workload-adaptive scaling. Due to the dynamic
cloud environment, two types of uncertainties exist in the
application workload: (1) the type of workload, such as Fig. 1’s
three types of workload; (2) the volume of workload, which is
denoted in terms of the arrival rate of incoming requests, namely
the number of incoming requests per time unit. In this context,
the elastic scaling must be adaptive to the changing workload, and
such adaptive scaling has triple meanings. First of all, bottleneck
tiers of applications should be automatically identified both for
scaling up and down. Secondly, scaling should be performed as
R. Han et al. / Future Generation Computer Systems ( ) – 3
an iterative process because fixing a bottleneck tier may create
another bottleneck at a different tier of the application. For
instance, in Fig. 1(d)’s workload, the bottleneck is shifted to the
storage tier if multiple Apache and Tomcat servers are added to
the service tier. Finally, agile scaling is needed to rapidly restore
acceptable application performance.
In the remainder of this paper we develop a framework and
algorithms that address both challenges effectively.We implement
and evaluate our approach using the IC-Cloud platform [17] as
an example. The advantage of using the IC-Cloud platform is
that supports a fine-grained pricing strategy (e.g., VM instances
are charged by minute rather than by the hour as in Amazon
WS) which simplifies the evaluation. However, the approach
and algorithms are generic and can be applied on most IaaS
environments.
3. Related work
3.1. Traditional scaling techniques before clouds
Scaling of applications has been studied extensively before
clouds. Early work considers single-tier applications and focuses
on transforming performance targets into underlying computing
resources such as CPU and memory [4–6]. Further investigations
classify an application into multiple tiers [7–10]. They then break
down the end-to-end response time by each tier and conduct the
worst-case capacity estimation to ensure applications meeting the
peak workload. Overall, the single-tier model can be viewed as a
special case of a multi-tier model and the latter model can guide
the scaling in a more accurate way.
However, although scaling of traditional applications, which
are often hosted on physical servers, shares many commonalities
with that of cloud applications, these two types of scaling
technique have different emphases. Conventional techniques
mainly concentrate on how to schedule compute nodes to meet
the QoS requirements of applications by predicting their long-
term demand changes. In contrast, clouds focus on providing
metered resources on-demand and on quickly scaling applications
up and down whenever application demand changes. Further
investigations, therefore, are needed to address the challenges
brought about by this requirement for high elasticity. For example,
a CloudSim toolkit is proposed to simulate an application in
order to accurately estimate its required resources before the
actual deployment in clouds [20]. Another example, Merino et al.
introduce a hybrid grid-cloud architecture and propose a market-
based economic mechanism to assist grid users in performing
scaling with cloud resources [21].
3.2. Policy-based scaling techniques
At present, numerous efforts have contributed to scale cloud
applications. Some resource provisioning systems [22], frame-
works [23,24] and lifecyclemanagement toolkits [25] are proposed
to manage cloud resources based on the idea of autonomic con-
trol [26]. Rather than developing concrete scaling methods, these
studies generally discuss higher-level concerns in building an ef-
fective provisioning system. Examples are performance metrics
used in resource allocation, service-level agreement (SLA) analyser,
performance monitor and VM allocator.
Most of the cloud providers (e.g., Amazon WS [1]) and vendors
(e.g., RightScale [11]) employ pre-defined policies (or rules) to
guide application scaling. Taking RightScale [11] as an example,
application owners need to manually define an application’s rules
for triggering scaling after its deployment. These rules specify the
minimum and maximum number of servers in the application,
the condition to scale these servers, the number of servers in
each scaling and even the scaling speed. Server monitor triggers
these rules to performscaling. Inheriting fromRightscale, UniCloud
extends the policy-based scaling by considering further issues such
as work priority and CPU speed [12]. In addition, Nathania et al.
introduce four types of policy to manage the allocation of VMs for
different types of applications [13].
Policy-based scaling allocates additional servers in scaling up
whenever some performance metrics exceed a threshold and
removes redundant servers in scaling down whenever these
metrics are less than a threshold. This scaling mechanism assumes
that application owners have the relevant knowledge of the
application being executed to define proper policies and this
assumption is sometimes not applicable to application owners.
In addition, this kind of scaling is designed to meet applications’
QoS requirements, and cost-effective resource provision is not
achievable in many cases.
3.3. Scaling techniques using analytical models
Many researchers apply the analytical modelling technique to
help application ownersmake scaling decisions by informing them
of the performance analysis results of applications. Xiong et al.
model an application by a network of queuingmodels and conduct
the performance analysis to show relationships among workloads,
server number and QoS level [27]. In [28], Ghosh et al. divide an
application into three types of sub analytic model: the resource
provisioning decision model, VM provisioning model and run-
timemodel. By iteratively solving each individual sub-model, their
analysis obtains two results: response time and service availability.
In addition, Ghosh et al. utilise a stochastic reward net to model
an application and give two analysis results: job rejection rate and
response delay [29]. In [30], Pal et al. propose a pricing framework
with economic models designed for multiple cloud providers in
the marketplace, where each cloud provider is modelled as a
queueing system. Using this queueing system, the framework aims
at informing application owners of the price and its related QoS
level. In [31], Huu et al. introduce several network provisioning
strategies based on a cost estimation model. These strategies are
used by application owners to predict the amount of resources and
their deploying cost for an application.
Moreover, a variety of application scaling approaches are
proposed using the analysis results of queueing models. In [14],
Bacigalupo et al. model an application by a queueing model with
three tiers, namely application, database and database disk tiers.
Each tier is then solved to analyse the mean response time,
throughput and utilisation of a server. Using these results, a
resource management algorithm is proposed to scale applications
in dynamic-urgent clouds.
Similar to [7–10], Bi et al. break down an application’s end-to-
end response time to each tier [15]. They then calculate the number
of servers allocated to the application subject to constraints of
average response time and arrival rate.
In [16], Hu et al. consider two allocation strategies using
queueing models: (1) shared allocation (SA) strategy where all
incoming requests have the same queuing; (2) dedicated allocation
(DA) strategywhere requestswith different arrival rate are divided
into multiple queues. An algorithm is then proposed to decide
which strategy (SA or DA) requires the smaller number of servers
to satisfy the QoS requirement.
Other stochastic models are also applied in guiding the scaling
of applications. In [32], Iqbala et al. present a methodology
supporting both the scaling up and down of a two-tier web
application. Scaling up is based on a reactive model that actively
profiles CPU utilisations of VMs. Scaling down uses a regression-
model-based predictive mechanism. In addition, Li et al. use
a network flow model to analyse applications and introduce
an approach to assist application owners in making a trade-off
between cost and QoS [33].
4 R. Han et al. / Future Generation Computer Systems ( ) –
To the best of our knowledge, previous scaling techniques
either provide general information to application owners and rely
on them to make proper scaling decisions [27–31], or propose
scaling approaches using the analysis results of analytical models
[7–10,14–16]. This sort of investigation has solved the scaling of
applications to meet their QoS satisfactorily. However, existing
scaling approaches rarely consider the equally important issue
of cloud computing—the cost. Applications deployed in a highly
scalable cloud environment require not just good performance but
also cost-efficient resource provision. An elastic scaling approach
formore efficient cloud elasticitymanagement, therefore, could be
a desirable advance.
4. A system to support elastic scaling
In this section, we first present an overview of the properties
of multi-tier applications (Section 4.1). A platform, called iSSe, is
then introduced to support the elastic scaling of these applications
(Section 4.2). Finally, we explain how iSSe achieves the automation
of application scaling (Section 4.3).
4.1. Multi-tier cloud applications
A cloud application can either be an infrastructure application
or an end-user application [34]. Examples of infrastructure appli-
cations are DNS servers, email servers or databases. Applications
of this sort often have simple structures, such as having one or two
tiers. By contrast, the structure of an end-user application is more
complex. For example, the e-commerce website in Fig. 1 has five
tiers. This work uses an end-user application as an example be-
cause it can also incorporate the simpler infrastructure application
scenario.
In amulti-tier application, servers are categorized into different
tiers according to their functionalities, as listed in Fig. 2: servers
at the two load balancing (LB) tiers, such as HAProxy and Amoeba,
distribute requests to servers at the service or storage tiers; servers
at the service tier, such as Apache and Tomcat, are responsible
for handling HTTP requests and implementing business logic; and
servers at the storage tier, such as the MySQL database, are used
for managing application data. In addition, servers at the service or
storage tier can be further divided into different sub-tiers.
Typically, each application has a set of demands and constraints
specified by the application owner in the form of a SLA. A
performance demand is defined by the maximum end-to-end
response time for a request. This time can either be an average
response time or a high percentile of response time distribution
(e.g., 90% of the response times should be less than 2 s). A
cost constraint is the budget of the total application deployment.
In addition, each tier has a resource constraint that restricts
the maximum number of servers in this tier. For instance, the
maximum number of Tomcat servers is 10.
Definition 1 (A Multi-tier Application). A multi-tier application
consists of twoparts: (1) the server set S, including all servers of the
application, and (2) the demand set D, capturing the requirements
specified in the SLA.
The multi-tier architecture guarantees the modularity of
cloud applications and facilitates the control of their tiers. An
application’s server set can be divided into multiple subsets and
each subset consists of servers belonging to the same tier. Each
server is marked by a unique tier id. For instance, in Fig. 1, for
an e-commerce website, the tier ids of HAProxy, Apache, Tomcat,
Amoeba and MySQL are 1–5, respectively. Starting from HAProxy,
which acts as the end users’ communication interface, servers at
each tier first receive in-processing requests from the previous tier,
process these requests locally and then transmit them to the next
tier.
Fig. 2. The multi-tier architecture of cloud applications.
Fig. 3. The architecture of iSSe.
Definition 2 (Server’s Tier Id). In am-tier application, each server s
has a unique tier ID, denoted by id(s). Servers’ tier ids are numbered
consecutively from 1 tom according to these servers’ tier types: LB
tier for Service, Service tier, LB tier for Storage and Storage tier.
Definition 3 (Server Subsets). In am-tier application, the server set
S can be divided into m server subsets: S1 ∪ S2 ∪ · · · ∪ Sm, which
are sorted in strictly ascending order according to the tier id of
their servers. That is, each subset Si consists of servers belonging
to tier i (i = 1, . . . ,m) and for any pair of servers s and s′, we have
id(s) < id(s′) if s ∈ Si and s′ ∈ Si+j (0 < j ≤ m − i).
4.2. iSSe to support elastic scaling
As shown in Fig. 3, iSSe acts as middleware between cloud
providers and application owners. The IaaSUser Portal of iSSe assists
application owners to conveniently provide services to application
end users. Using this portal, application owners can specify the
required response time for their applications, select the needed
servers and their configuration from the Repository of Servers, and
get notifiedwhenworkload violation happens and applications are
scaled up (down).
In addition to the IaaS User portal, the other four service
components in iSSe work together to support elastic scaling. The
Monitoring Service monitors each running application using two
types of monitor (see Fig. 4). The first type of monitor is the
entry monitor, which examines the incoming requests over a finite
interval (e.g., 60 s) and records information such as the request
arrival rate. This information is used to decide whether a scaling
up (or down) is needed. For instance, scaling up is triggered if
the observed response time exceeds the response time threshold
defined by the application owner. The second type ofmonitor is the
server monitor installed on each server. This monitor examines the
server’s resource usage (e.g., CPU utilisation), analyses tier-specific
values, such as response time, and collects other server execution
logs. All monitoring information is stored in a database. When
a scaling is triggered, the Capacity Estimation Service conducts
a scaling algorithm (e.g., CAS algorithm explained in Section 5)
and estimates the number of servers to be scaled using the
information in the database. Subsequently, the Deployment Service
R. Han et al. / Future Generation Computer Systems ( ) – 5
Fig. 4. The monitoring service of iSSe.
Fig. 5. The automatic addition of a Tomcat server in the Deployment Service.
automatically implements the scaling of these servers by calling
interfaces in IC Cloud [17]. In the scaling process, all information
related to the multi-tier configuration is obtained from servers’
specifications.
4.3. Server specification and automation of application scaling
In iSSe, each server is installed in a stand alone VM. As shown
in Fig. 4, this VM consists of installed software such as Tomcat
software, as well as a server monitor and pre-loaded auto-running
scripts used in automating the scaling process.
An XML-based specification is available to describe the key
aspects of a single server by extending the Open Virtualization
Format (OVF) open standard [34]. The OVF standard is widely
adopted by key industry vendors, such as VMware, IBM and
Oracle, to describe their VMs. Using this specification facilitates
scaling in several ways: (1) different features of the server
are described concisely and accurately; (2) automatic content
validation and software licensing are supported based on an
industry standard; (3) single server specifications can be saved
as standard templates and a complex multi-tier application can
be constructed by multiple interdependent templates; (4) the
specification is independent of cloud vendor and platform.
Fig. 5 shows an example specification of the Tomcat server.
This specification has four sections. The first section lists the
server’s basic information, such as its tier id and its replication
constraints. The subsequent three sections define the server’s VM
configuration, software user settings and links to other servers,
respectively.
In the Deployment Service, the automatic scaling indicates that
the scaling process, namely addition, removal and resource modifi-
cation of servers, can be automatically executed by interpreting the
servers’ specifications. Automating the addition activity involves
three steps, as shown in Fig. 5. In step 1, theDeployment Service first
generates a Tomcat VM image, as specified in the Basic Informa-
tion section and VM Configuration section. In step 2, the VM is first
started, after which the auto-running scripts preloaded in this VM
use the parameters in the Software User Setting section to configure
the Tomcat software: the user name, password and port number of
the Tomcat are set. At step 3, the Tomcat is linked to its input load-
balancing server HAProxy-01, as specified in the Linking section.
The Addition operations take a few minutes to achieve.
Typically, the generation of a VM image (step 1) can be finished
within a few seconds. This is because after a server is packaged,
most existing cloud platforms (including IC-Cloud and Amazon
WS) usually keep a certain number of images ready for use. In
addition, step 2 and 3 can be completed within 1 or 2 min. The
time is mainly consumed in starting up the VM and executing the
scripts to configure the server software. However, there are two
situations when the addition activity may need a slightly longer
6 R. Han et al. / Future Generation Computer Systems ( ) –
time to complete. The first situation is when a server at the storage
tier is to be added. In this case, the server may need some time to
update/replicate data. An example is when a newly added MySQL
Slave needs to synchronize with the MySQL Master. The second
situation arises due to the auto-running script of a LB server (e.g., a
HAProxy), which executes once every few minutes. Hence when a
new Service/Storage server (e.g., Tomcat) is added, it needs to wait
until the LB server’s script runs again to detect and register this
new server.
Automating the removal activity is achieved by conducting the
reverse operations of the addition activity: the running Tomcat
server is disconnected from its input server HAProxy and removed
from the application. Note that this server’s running VM instance is
only shut downwhen its existing billing period ends. For example,
most mainstream providers, such Amazon WS, today bill their
users by the hour. However, the CAS algorithm can charge VM
instances in a finer granularity than per hour in order to achieve
cost-efficient scaling. For example, a server s1 is added to an
application at t = 0 (time unit is minute), it is removed at t = 5
and added at t = 15; it is removed again at t = 30 and added back
to the application at t = 45. Assuming that server s1 is charged by
minute in the CAS framework, its cost is less than a server s2 that
keeps running for an hour: c(s1) = (35/60)c(s2) because server
s1 only runs, and is charged for, 35 min. This fine-grained pricing
strategy is supported in the IC-Cloud platform (e.g., a VM instance
is billed by the minute).
The resource modification activity is conducted if information in
the Tomcat’s specification is updated. This activity reconfigures the
VM, resets the Tomcat’s user settings and links to related servers.
The reconfiguration of the VM is based on the observation that
multiple basic (smallest) resource units can constitute a VM and so
the VM configuration can be modified by changing the number of
these units. For instance, one basic resource unit can be equivalent
to one VMwith one 40 GHz CPU, 1 GB memory and 10 GB storage.
If each server is hosted in a VM with one basic resource unit,
increasing or decreasing a unit in the resource modification activity
could have equivalent effect to addition or removal of a server.
The fine-grain management of the VM and fine grained resource-
level scaling are discussed in our complementary work [35], which
models each basic resource unit by a vector consisting of multiple
dimensions, including CPU, size of memory, I/O amount, allowing
both the performance and cost of individual units to be analysed
precisely.
5. The CAS algorithm
In this section, we provide an overview of the CAS algorithm
(Section 5.1) and introduce two cost-aware criteria to guide scaling
up and down of applications (Section 5.2). Using these criteria,
we then present two capacity estimation algorithms: the Cost-
Aware-Capacity-Estimation (CACE)-For-Scaling-Up (Section 5.3)
and CACE-For-Scaling-Down (Section 5.4). We also define the
performance metrics to evaluate the behaviour of the scaled
applications (Section 5.5).
5.1. The algorithm overview
For clarity, Table 1 lists the main parameters of a m-tier
application used throughout the CAS algorithm. The values of
these parameters are obtained in a variety of ways: (1) the
single server deployment cost (price) is decided by the cloud
providers (vendors); (2) the required response time and budget are
specified by the application owner in a SLA; (3) interarrival time
(i.e., the reciprocal of its arrival rate) of end users’ requests are
collected online by the entry monitors; (4) each server’s service
time distribution, namely its mean (value) and variance, and the
branch probabilities between different tiers are analysed by offline
profiling of user behaviours. The data for the offline profiling can
be obtained through simulation of servers (e.g., simulating by
CloudSim [36]), analysing servers’ execution logs and consulting
related deployment documents.
The CAS algorithm, detailed below, is started after an applica-
tion is initially deployed and keeps running until the application
is terminated (line 2–11). Whenever a change in the incoming re-
quests rate is detected (line 6 or 9), the algorithm triggers a capac-
ity estimation, either a CACE-For-Scaling-Up algorithm (line 7) or
CACE-For-Scaling-Down (line 10) to obtain the updated server set
S. Using the result of the capacity estimation, the algorithm adds
more servers (line 8) or removes redundant servers (line 11) in par-
allel to quickly scale the applicationwithin a fewminutes. The time
complexity of each scaling is decided by the capacity estimation
algorithms introduced in the following sections.
CAS Algorithm
Input: c(s), S,D.
1. Begin
2. while (the application is not completed)
3. Monitor λm, λv once every few minutes;
4. Let λm′ be the last monitored mean interarrival time;
5. Let S ′
= S ′
1

S ′
2

, . . . ,

S ′
m be the server set before scaling;
6. if λm < λm′, then // the workload increases.
7. S = CACE-For-Scaling-Up (c(s), S ′,D, λm, λv);
8. Simultaneously add each server s, where s ∈ S i and s /∈ S ′
i ;
//scaling up
9. else if λm > λm′, then // the workload decreases.
10. S = CACE-For-Scaling-Down (c(s), S ′,D, λm, λv);
11. Simultaneously remove each server s, where s /∈ Si and s ∈ S ′
i .
//scaling down
12.End
The CAS algorithm applies an automatic reactive scaling mech-
anism similar to mechanisms applied by Amazon WS [1] and
RightScale [11], but the scaling mechanism in our work needs
no pre-defined rules to trigger scaling. In contrast, the CAS algo-
rithm relies on online monitors to detect the changes in work-
loads and perform corresponding scaling. In addition, traditional
predictive scaling methods are motivated by the long-term work-
load variations, which can be predicted using application profiling
[7–10]. These methods usually allocate servers to an application
well ahead of the expected workload increase because computa-
tional resources are difficult to obtain on demand in traditional
infrastructures such as grids. In contrast, cloud infrastructures
providemetered resources on demand.Within this context, the re-
active mechanism is applied in this work to quickly scale applica-
tions up anddownwhenever the user demand changes. Thismakes
the CAS algorithm suitable for scaling applications with both long-
term and predictable workload variations and short-term and
unpredictable variations.
5.2. Criteria for capacity estimation
In the scaling up or down of an application, addition or removal
of a server influences both the response time and deployment
cost of the application. The CAS algorithm aims at spending as
little cost as possible to meet the required response time. To this
aim, the cost-aware criteria are designed to analyse the effect of
cost on every unit of response time. To achieve this we develop
a performance model for the multi-tier applications based on
queueing theory.
Typically, a queueing system can be described using A/S/n,
where A represents the arrival process, S represents the distribu-
tion of service time and n is the number of servers [37]. In the ex-
ample queueing systemof Fig. 6(a),A/S/p = G/G/1 (G for general).
R. Han et al. / Future Generation Computer Systems ( ) – 7
Table 1
Parameters of the CAS algorithm.
Parameters Descriptions Data source
c(s) Single server s’s deployment cost Cloud providers
r sla The required end-to-end response time SLA
cb The budget of the total deployment
λm Mean (value) of interarrival time of end users’ requests Online measurement
λv Variance of interarrival time of end users’ requests
xm(s) Mean of server s’s service time
Offline profiling
xv(s) Variance of server s’s service time
pji The routing probability that a request leaves tier j and proceeds to tier i
p0i The probability that a request enters tier i from end-users
pi0 The probability that a request at tier i leaves the application
Fig. 6. Examples of G/G/1 and G/G/n queueing systems.
A G/G/1 queueing system includes one server s and one queue,
where both request interarrival time λ and server s’s service time
follow arbitrary distributions. Note that a queueing system has
three optional components: B/K/SD, where B represents the queue
capacity, K represents the size of incoming requests and SD stands
for the service discipline of waiting requests. Generally, it is as-
sumed that B/K/ = ∞/∞/FIFO.
The G/G/1 queueing system of Fig. 6(a) is used to model a
Tomcat server in a multi-tier application. It is clear that if server
s’s average service time xm(s) is longer than its average request
interarrival time λm(s) (i.e., xm(s)/λm(s) > 1) in the queueing
system, the system is unstable: the queue will become longer
and longer. In this work, we assume that each G/G/1 queueing
system is in a stable state (i.e., xm(s)/λm(s) < 1 and the queue
is at equilibrium). Based on the steady-state assumption, we can
analyse the interdeparture time of the queueing system’s output
requests. First, the mean of the interdeparture time dm(s) is
equivalent to the mean of the interarrival time λm(s) : dm(s) =
λm(s). Secondly, the variance of the interdeparture time dv(s)
is decided by two independent random variables in the system,
namely the variances of the interarrival time λv(s) and the service
time xv(s). A simple way to calculate the approximate variance of
the interdeparture time is: dv(s) ≈ (1 − (xm(s)/λm(s))2)λv(s) +
(xm(s)/λm(s))2xv(s). This approximation is reasonable based on
the observation that if the system is under light load (i.e., xm(s)
is significantly smaller than λm(s)), xm(s)/λm(s) is close to 0 and
dm(s) is approximately equal to the interarrival time λv(s). On the
other hand, if the system is under heavy load (i.e., xm(s) is very close
toλm(s)), xm(s)/λm(s) is close to 1 and dm(s) is approximately equal
to the service time xv(s).
In addition, the G/G/n queueing system extends the G/G/1
system such that it has n parallel and independent servers
(homogeneous or heterogeneous) and the above analysis of
departure process is still applicable. TheG/G/n queueing system of
Fig. 6(b) is used tomodel a tier of Tomcat servers. For convenience,
we assume that servers in the same tier are homogeneous
(relaxation of the assumption is possible and the proposed
estimation algorithm is still applicable).
In a multi-tier application, let Si be the server set at tier i (i =
1, . . . ,m) and let the mean and variance of server s’s service
time at this tier be xm(s) and xv(s). Tier i is modelled as a G/G/n
queueing system, where n = |Si| is the number of servers. This
G/G/n queueing system is n times faster than a single server sj
at this tier, and the mean and variance of tier i’s service time is
xm(Si) = xm(s)/|Si| and xv(Si) = xv(s)/|Si|2, respectively. We can
then estimate the response timeof tier iusing the queueing system.
Definition 4 (A Tier’s Response Time). Consider a tier i in a multi-
tier application that is modelled as a G/G/n queueing system. A
request’s expected response time of this tier, denoted by r(Si), is
the sumof itswaiting time in the queue and themean service time:
r(Si) = w(Si) + xm(Si),
where w(Si) is the request’s waiting time in the queueing and
w(Si) =
(λv(Si)+xv(Si))
2λm(Si)(1−xm(Si)/λm(Si))
[37]. We then get tier i’s response
time based on the worst cast estimation:
r(Si) =
(λv(Si) + xv(Si))
2λm(Si)(1 − xm(Si)/λm(Si))
+ xm(Si), (1)
where λm(Si) and λv(Si) are the mean and variance of the interar-
rival time of incoming requests at tier i.
Furthermore, the whole m-tier application is modelled as a
network ofm G/G/n queueing systems and each queueing system
represents a tier in the application. Using this open queueing
network, the interarrival time of each tier’s incoming requests
can be analysed. Take Fig. 1’s 5-tier e-commerce website as an
example, Fig. 7 shows three types of G/G/n queueing systems
in the queueing network. In Fig. 7(a), the queueing system of
the first tier (HAProxy servers) only receives requests from end
users, so this system has the same request arrival rate 1/λ (i.e.,
the reciprocal of interarrival time λ) with the application. In
addition, tier 1’s departure requests d1 can go to any tier i in the
application with probability p1i and
5
i=1 p1i = 1. In Fig. 7(b),
the request arrival rate of tier 3’s queueing system is the sum
of arrivals from all 5 tiers of the application (including tier 3
itself): 1/λ3 =
5
i=1(pi3/di), where the arrival rate of tier i (i.e.,
pi3/di) is the product of the routing probability pi3 that a request
leaves tier i and proceeds to tier 3 and tier i’s departure rate
1/di. Hence we get λm(S3) = (
5
i=1(pi3/d
m(Si)))−1 and λv(Si) =
(
5
i=1(pi3
2/dm(Si)))−1. In addition, Fig. 7(c) shows the queueing
system of the last tier (MySQL), in which the departure requests
can either leave the application (with probability p50) or go to other
tiers (with probability
5
i=1 p5i = 1 − p50).
8 R. Han et al. / Future Generation Computer Systems ( ) –
(a) The G/G/n queueing system for the first tier of the application. (b) The G/G/n queueing system for the third tier of the application.
(c) The G/G/n queueing system for the last tier of the application.
Fig. 7. A G/G/n queueing system of tier i in a multi-tier application.
(a) A example queueing network of the ‘‘Homepage browsing’’ web interaction.
(b) A example queueing network of the ‘‘Buy request’’ web interaction.
Fig. 8. Two example queueing networks of the multi-tier application.
In conclusion, in the open queueing network of a m-tier
application, new end-user requests enter the network from the
first tier. When they are processed, they are immediately preceded
to other tiers. The requests finally leave the application from its last
tier after traversing all them tiers. The application’s response time,
therefore, is the sum of each tier’s response time according to the
general response time law [38].
Definition 5 (Total Response Time of a Multi-Tier Application). The
total response time of a m-tier application, denoted by ra(S), is
equal to the sum of response time at each tier:
ra(S) =
m
i=1
r(Si). (2)
In the above analysis, we assume that all requests belong to the
same class. This means they share two important characteristics
in a queueing network, namely the same service time for the
same server and the same route through the network. However,
in reality, there are multiple classes of requests that form different
types of workload. For example, in Fig. 1’s e-commerce website,
different web interactions have different classes of requests. Fig. 8
displays two queueing networks of this 5-tier application under
two different web interactions and the request flows of these two
queueing networks are displayed. In the ‘‘Homepage browsing’’
web interaction, end users mainly visit webpages hosted on
Apache and Tomcat servers. Hence the service time (time unit is
second) for the Apache server is 0.2, for the Tomcat server is 0.1 and
for other servers are close to 0. In addition, 1/2 of the departure
requests of the Tomcat server proceed to the tier of Amoeba and
the other 1/2 ones return to the tier of Apache. By contrast, in the
‘‘Buy request’’ web interaction, the requests of endusers aremainly
processed by Tomcat and MySQL servers. Hence the service time
for these two types of servers is 0.2 and for the other servers is
close 0. The routes of requests are also different between the two
web interactions. The expected response time of the application,
therefore, is the probability integration of the total response time
(Definition 5) of different classes of requests.
Definition 6 (The Expected Response Time of a Multi-Tier Applica-
tion). Consider a multi-tier application with k classes of requests.
Let the application’s total response time in class i is rai (S) and the
proportion of class i be pct i where
k
i=1 pct i = 1. The expected re-
sponse time of the application, denoted by r t(S), is the probability
integration of the total response time of k classes of request:
r t(S) =
k
i=1
rai (S)pct i. (3)
Using the analysis results of the queueing theory, the criterion
for the CACE-For-Scaling-Up algorithm is designed to measure the
R. Han et al. / Future Generation Computer Systems ( ) – 9
cost spent in adding a server divided by the decreased response
time because of this addition. Hence this criterion is called the
consumed cost/decreased response time (CC/DRT) ratio.
Definition 7 (CC/DRT Ratio). Tier i’s CC/DRT ratio, denoted by
RCC/DRT , is the cost spent per unit time in reducing response time
through adding a server s to tier i:
RCC/DRT (Si) = c(s)/(r(Si) − r(Si ∪ {s})). (4)
The criterion for the CACE-For-Scaling-Down algorithm, called
Saved Cost/Increased Response Time (SC/IRT) ratio (Definition 8),
is the cost saved by removing a server divided by the increased
response time due to this removal.
Definition 8 (SC/IRT Ratio). Tier i’s SC/IRT ratio, denoted by RSC/IRT ,
is the cost saved per unit time in increasing response time through
removing a server s from tier i:
RSC/IRT (Si) = c(s)/(r(Si \ {s}) − r(Si)). (5)
5.3. Capacity estimation for scaling up
The CACE-For-Scaling-Up algorithm aims at adding servers to
an application to reduce its response time below a specified target
threshold, while keeping the deployment cost as low as possible.
Given this motivation, the algorithm judges the tier with the
smallest CC/DRT ratio as the bottleneck tier where a server needs
to be added. Compared to other tiers, addition of servers to this
tier can decrease the response time with the smallest cost per unit
time.
A detailed algorithm is given below. The algorithm first builds
a candidate server set SC . The candidate set consists of all eligible
tiers’ server subsets (an eligible tier is the tier that can be added at
least one server in scaling up). The initial candidate set takes each
tier’s server subset as its element (line 2). The algorithm iteratively
executes under the condition that the candidate set SC is not empty
and the total response time r t(S) is greater than the required time
r sla (line 4–15). In each loop, the algorithm first tries to find the
tiers where adding a server can make r t(S) ≤ r sla and ends the
capacity estimation (line 5’s S∗ is the server set of these tiers). If
one ormultiple tiers are found (line 6), the tier whose single server
is cheapest is selected as the bottleneck tier (line 7); otherwise,
the algorithm selects the bottleneck tier with the smallest CC/DRT
ratio (line 9 and 10). Subsequently, the algorithm judges whether
adding a server to the bottleneck tier violates any constraint (line
11). If the addition is feasible, a server is added (line 12); otherwise,
the selected tier is viewed as ineligible to receive an added server
and its server subset is removed from the candidate list (line 14).
The constraints checked in line 11 include the constraints
specified in the SLA and servers’ own constraints. Examples of
the former constraints are the cost constraint (the application’s
deployment budget) and resource constraint (each tier’smaximum
number of servers). An example of the latter constraint is the
server’s replication constraint. For instance, there is at most
one MySQL Master server in an application, so MySQL Master’s
replication constraint is 1.
Note that if r t(S) is still larger than the required time r sla while
adding a server to any tier of the application is infeasible (line
18), the scaling process is halted and an exception handling is
triggered to inform the application owner (line 19). The application
owner can either relax the violated constraints or modify the
response time target. For example, if the cost constraint is violated
(i.e., adding any server to the application exceeds the deployment
budget), the application owner can either increase the budget and
resume the scaling process, or increase the required time r sla to
accept the existing response time r t(S) and stop the scaling up.
CACE-For-Scaling-Up Algorithm
Input: c(s), S,D, λm, λv .
Output: updated S.
1. Begin
2. SC = {S1, S2, . . . , Sm}; // the original candidate server set
3. Compute r t(S) using Eqs. (1)–(3);
4. while (SC is not empty and r t(S) > r sla) do
5. Find subset S∗ from SC ;
6. if (S∗ is not empty), then
7. Select Si from S∗ with the smallest c(sj), where sj ∈ Si;
8. else
9. Compute each cc(Sk), where Sk ∈ SC , using Eq. (4);
10. Select Si from SC with the smallest RCC/DRT (Si);
11. if (Add a server s to tier k is feasible), then
12. S = S ∪ {s}; // add s to tier i
13. else
14. SC = SC\Si; // remove server subset Si from the candidate
set
15. Compute r t(S) using Eqs. (1)–(3);
16. if (r t(S) < r sla), then
17. Return S.
18. else // r t(S) > r sla and SC is empty
19. Halt the scaling process and trigger an exceptional handling.
20.End
Theorem 1. The time complexity of the CACE-For-Scaling-Up algo-
rithm is O(m2), wherem is the number of tiers for the application. Note
that in practice, m is usually a small number, ranging from 1 to 8.
Proof. In the CACE-For-Scaling-Up algorithm, each time we
conduct the estimation loop (line 4–15), it takes O(m) to complete
the traversal of allm tiers to find the bottleneck tier (line 7 and 10).
Other operations in the loop can be done in constant time. In each
loop, either a server is added (line 12) or a server subset is removed
(line 14). Hence the algorithm can be completed within O(C + m)
loops, where it takes O(C) to add C servers (C is a positive constant
that is usually less than 20) and O(m) to remove all subsets from
the candidate list. The total time complexity, therefore, is O(m2).
Theorem is proved. 
5.4. Capacity estimation for scaling down
The CACE-For-Scaling-Down algorithm aims at removing
servers from an application to reduce the cost as much as possi-
ble while still meeting the application response time. To this aim,
this algorithm judges the tier with the largest SC/IRT ratio as the
bottleneck tier where a server needs to be removed, because this
removal can save the maximum cost per unit of response time
increased.
A detailed CACE-For-Scaling-Down algorithm is given below.
The algorithm iteratively executes until no redundant servers can
be removed. In each loop, the algorithm first find all ineligible
tiers, where removing one server from any of these tiers would
make the total response time exceed the required time in the
SLA. The algorithm then removes all ineligible tiers’ server subsets
from the candidate server set (line 5). Consequently, the remaining
tiers can be removed by at least one server. From these tiers,
the algorithm selects the bottleneck tier with the largest SC/IRT
ratio and removes a server (line 6 and 7). Note that the algorithm
also checks the constraints which may forbid removal (line 9).
For example, the HAProxy acts as the end users’ communication
interface, so the server set of HAProxy must have at least one
server.
CACE-For-Scaling-Down Algorithm
Input: c(s), S,D, λm, λv .
Output: Updated S.
10 R. Han et al. / Future Generation Computer Systems ( ) –
1. Begin
2. SC = {S1, S2, . . . , Sm}; // the original candidate server set
3. Compute r t(S) using Eqs. (1)–(3);
4. while (SC is not empty and r t(S)≤ r sla) do
5. Find and remove ineligible server subset from SC ;
6. Compute each cs(Sk), where Sk ∈ SC , using Eq. (5);
7. Select Si from SC with the largest RSC/IRT (Si);
8. if (Remove a server s from tier i is feasible), then
9. S = S\{s}; // remove s from tier i
10. else
11. SC = SC\Si; // remove subset Si from the candidate server
set
12. Compute r t(S) using Eqs. (1)–(3);
13. Return S.
14.End
Theorem 2. The time complexity of the CACE-For-Scaling-Down
algorithm is O(m2), where m is application A’s tier number.
Proof. Similar to the CACE-For-Scaling-Up algorithm, it is not
difficult to prove that the CACE-For-Scaling-Down algorithm has
O(m + C) loops in maximum and each loop takes O(m) to
complete all operations. Hence the total time complexity is O(m2).
Theorem 2 is proved. 
5.5. Performance metrics for comparing applications
In an elastic cloud environment, computing resources are
consumed on-demand similar to traditional utilities such as
water and electricity [19]. In this context, we argue that the
cost/performance ratio is the key factor for application owners’
scaling decisions. An application’s cost includes the expense
of deploying all servers (Definition 9) and these servers are
usually charged in pay-as-you-go pricing model in clouds. The
performance is measured by throughput with a pre-defined
response time constraint (Definition 10). The cost/performance
ratiomeasures the cost divided by the performance (Definition 11)
and it is used as ametric for comparing two scaled applications. For
example, two e-commerce websites can be compared in terms of
cents spent perminute for processing every 100 requests under the
constraint that the average response time is below 2.0 s.
Definition 9 (Total Deployment Cost). In a m-tier application with
server set S, the total cost needed to deploy the application is
denoted by ct(S), where ct(S) =
m
i=1 |Si|c(sj) (sj ∈ Si).
Definition 10 (Performance). The performance of an application
is measured in terms of throughput t (the number of processed
requests per unit of time) under the constraint that response time
specified in the SLA is satisfied [38].
Definition 11 (Cost/Performance Ratio). The Cost/Performance ra-
tio of an application, denoted by RC/P , is the cost spent per unit of
performance: RC/P
= ct(S)/t .
6. Experimental evaluation
In this section, we first introduce the experimental set-up
(Section 6.1), following the results of experimental evaluation.
The evaluation is designed to illustrate the effectiveness of our
CAS algorithm in adapting changing workloads by effectively
scaling up and down applications (Section 6.2). More importantly,
the CAS algorithm’s salient feature in delivering cost-efficient
services is demonstrated by comparison with existing techniques
(Section 6.3).
6.1. Experimental set-up
6.1.1. Hardware and software environment
iSSe was implemented as a full working system on top of the
IC-Cloud infrastructure. In [39], we describe the full functionality
of a basic engine that provides a convenient deployment portal
and repository of servers to automate both applications’ initial
deployment and scaling. Similar to [7–10,15], the basic capacity
estimation is applied in [39], which breaks down the end-to-end
response time by each tier and calculates the tier’s server number
separately.
In our experiment, iSSe was tested in a data centre running
IC-Cloud platform [17]. This data centre has four PMs that share the
4.1 Tb centralised storage and are connected through a switched
gigabit Ethernet LAN. Each PM has 8 Quad-Core AMD processors
and 32 GB memory.
We tested an e-commerce website, implemented as an online
bookstore application according to the TPC-W industry standard
benchmark [18], in the experiment. For convenience, each server
of the application is installed on one dedicated VM running Linux
Centos 5.4. Different servers have different VM configuration
details, as listed in Table 2. In addition, two versions of the
MySQL database (i.e., MySQL Master and Slave) are implemented
to support the MySQL Master-Slave data replication model. For
instance, a MySQL Master is initially deployed and, when the
storage tier needs to be scaled up, several MySQL Slaves are added
and configured with replication from the MySQL Master.
6.1.2. Application logic implementation
We implement 14 web interactions of the online bookstore
application, each one differing from the others in terms of the
required server-side processing (i.e., service time and route of
requests). By mixing these interactions, we distinguish three types
of workload as representing the typical behaviour of customers, as
shown in Table 3: (1) the primarily ‘‘browsing’’ workload describes
the simultaneous execution of multiple http transactions on web
servers and the dynamic page generation by accessing the database
using application servers. This mixed workload mainly contains
interactions such as ‘‘Home’’, ‘‘Order Inquiry’’ and ‘‘Product Detail’’,
which stress servers at the Service tier (e.g., Apache and Tomcat)
and only make light and short database queries; (2) the primarily
‘‘ordering’’ workload stands for the accessing and updating of
databases. Thisworkload consists of interactions that need tomake
heavy database queries, mainly stressing MySQL servers at the
Storage tier; finally, (3) the typical ‘‘shopping’’ workload represents
the whole shopping process and comprises interactions that stress
both the Service and Storage tier.
To simulate the above workloads, we implement a client
emulator. After setting the test period, this emulator can simulate
a number of concurrent end users. Each end user continuously
generates a sequence of requests to stress the server-side
application. After a request is completed, the simulated end user
waits for a random interval before initiating the next request
to simulate actual end users’ thinking time. The probability of
initiating an interaction is controlled by a transition matrix that
specifies the probability of proceeding from one interaction to
another. In the evaluation, a number of VMs with 4 CPUs and
4 GB RAM are employed to run the emulator. This ensures that
the simulation clients are not the bottleneck in any experimental
evaluation.
6.2. Effectiveness of the CAS algorithm
This section demonstrates the effectiveness of our CAS algo-
rithm in scaling up and down applications to handle changing
R. Han et al. / Future Generation Computer Systems ( ) – 11
Table 2
Six types of servers’ deployment information.
Server name CPU RAM (GB) Software version Cost (cents/min) Replication constraint Resource constraint
HAProxy 2 2 Haproxy-1.4.8 0.34 3 1
Apache 2 2 Apache 2.2.20 0.34 Infinite 10
Tomcat 1 1 Tomcat 7.0.22 0.17 Infinite 10
Amoeba 2 2 Amoeba-mysql-1.3.1 0.34 3 1
MySQL master 4 4 MySQL 5.5 0.68 1 1
MySQL slaver 1 1 MySQL 5.5 0.17 Infinite 10
Table 3
Three types of workloads and their proportions of mixed interactions.
Web interactions Interaction proportion in the workload
Primarily ‘‘browsing’’ (%) Primarily ‘‘ordering’’ (%) Typical ‘‘shopping’’ (%)
Admin request 0.09 9.09 6.11
Admin response 0.10 9.10 6.12
Best seller 0.82 3.00 0.46
Buy confirm 0.69 9.20 9.18
Buy request 0.75 0.60 12.73
Customer registration 15.00 3.00 12.86
Homepage browsing 27.00 6.00 18.12
New product 1.00 3.00 0.46
Order display 0.25 0.66 0.22
Order inquiry 21.00 5.75 0.25
Product detail 21.00 12.00 12.35
Search request 11.00 18.00 6.06
Search result 0.60 12.00 9.08
Shopping cart 0.70 8.60 6.00
workload volume (Section 6.2.1) and types (Section 6.2.2). In the
experiment, the online bookstore application is initially deployed
in one HAProxy, Apache, Tomcat, Amoeba and MySQL Master. In
the SLA of this application, the required average end-to-end re-
sponse time r sla is assumed to be no more than 2 s and total de-
ployment budget cb is 5 cents/min.
6.2.1. Scaling for changing workload volume
In the first trial, we test the primarily ‘‘browsing’’ workload
using nine sessions: the first five sessions stepwise increase
the number of simulated end users to initiate scaling up and
the remaining four sessions gradually decrease this number to
trigger scaling down, as shown in Fig. 9(a). This variance of end
user numbers denotes the changing workload volume, and the
‘‘thinking time’’ between two requests randomly varies between 0
and 1 s. More concretely, the first session is generated at time= 0 s
and lasts 600 s. During this period, the application is monitored
once every 60 s and Fig. 9(b) displays 10 observed arrival rates of
incoming requests. These observed values can be used to derive the
mean λm and variance λv of the request’s interarrival time.
Fig. 10 demonstrates the fluctuation of the end-to-end response
time observed in the trial of the ‘‘browsing’’ workload. In the
first five sessions, the response time is violated whenever the
active session number is increased. For instance, when the session
number is increased to 400 at time = 600 s and saturates two
Service tiers. The scaling up is triggered and two Tomcat and one
Apache servers are added. The violation typically lasts for 1 or
2 min because the addition of new servers consumes some time.
By contrast, in the sixth to ninth sessions, the CAS algorithm scales
down the application while meeting the required response time.
Result. When the workload volume increases, the CAS algorithm
can scale up the application to restore the requested response time
within 1 or 2 min. On the other hand, the algorithm can scale
down the application when the workload volume decreases, while
maintaining the response time target.
6.2.2. Scaling for changing workload types
We repeat the last section’s trial to test all three types of
workloads, and their observed arrival rates are shown in Fig. 11.
Although these rates are similar to each other, workloads of
different type stress different tiers of the application.
Given the above workloads, Fig. 12 shows that the CAS algo-
rithm can scale up and down the application to meet the required
response time for the ‘‘ordering’’ and ‘‘shopping’’ workload. More-
over, Fig. 13 shows the number of servers deployed for each work-
load to support different session loads. This number adapts to
changes in workload types. For example, in most cases of scal-
ing up (down) for the ‘‘browsing’’ workload, the tiers of Tomcat
and Apache are saturated or idle and the number of these servers
changes with the session number (Fig. 13(a)). By contrast, the
MySQL Slave tier is influenced significantly by the session num-
ber in the ‘‘ordering’’ workload (Fig. 13(b)). Note that only three
types of server, namely Tomcat, Apache andMySQL Slave, are listed
because the number of other types of server does not change due
to their replication and resource constraints. In addition, Fig. 14
presents the total cost of deploying these servers and that this cost
is always kept within the budget.
Result. Our CAS algorithm is able to adapt with different types of
workload and identify the bottleneck tiers for each scaling. When
the scaling is performed, only the bottleneck tiers are scaled up and
down to maintain the response time target.
6.3. Comparison with existing scaling techniques
This section shows the effectiveness of our CAS algorithm in
scaling on response time (Section 6.3.2) and cost-efficient services
(Section 6.3.3).
6.3.1. Two categories of existing scaling technique
Typically, existing scaling techniques can be divided into two
categories.
In the first category, applications are scaled using pre-defined
polices [1,11–13]. These scaling algorithms can be termed Policy-
based scaling (PBS), and are used by many mainstream cloud
providers such as Amazon WS [1] and Rightscale [11]. In the
experiment, if the CPU utilisation of one type of server (e.g., Apache
or Tomcat) is larger than 80% for 2 min, one or more additional
12 R. Han et al. / Future Generation Computer Systems ( ) –
(a) Nine sessions and their number of concurrent end users.
(b) Requests’ arrival rate.
Fig. 9. Nine simulated sessions and observed request arrival rate of the ‘‘browsing’’ workload.
Fig. 10. The end-to-end response time in the trial of the ‘‘browsing’’ workload.
servers is added. On the other hand, if this utilisation is less than
50% for 2 min, redundant servers are removed.
In the second category, an application is modelled by a network
of queueing models including single or multiple tiers of servers.
Each server’s capacity is then analysed using the G/G/1 model.
Subsequently, each tier’s required server number is calculated by
breaking the application’s end-to-end response time into per-tier
response times. This type of scaling techniques [4–10,15,39],
therefore, is termed Tier-Dividing Scaling (TDS). The TDS algorithm
applies worst-case capacity estimation to deploy sufficient servers
capable of handling the peakworkload. In the experiment, the end-
to-end response time (i.e., 2.0 s) is broken down into three per-tier
response times, which are 20%, 40% and 40% for the tiers of Apache,
Tomcat and MySQL, respectively.
R. Han et al. / Future Generation Computer Systems ( ) – 13
Fig. 11. The request arrival rate in three types of workload.
(a) ‘‘Ordering’’ workload. (b) ‘‘Shopping’’ workload.
Fig. 12. The end-to-end response time in the trial of the ‘‘browsing’’ and ‘‘shopping’’ workloads.
(a) ‘‘Browsing’’ workload. (b) ‘‘Ordering’’ workload. (c) ‘‘Shopping’’ workload.
Fig. 13. Number of servers in three types of workloads.
(a) ‘‘Browsing’’ workload. (b) ‘‘Ordering’’ workload. (c) ‘‘Shopping’’ workload.
Fig. 14. The total deployment cost in three types of workload.
14 R. Han et al. / Future Generation Computer Systems ( ) –
Fig. 15. The end-to-end response time in the trial of the ‘‘browsing’’ workload using three algorithms.
Fig. 16. The proportion of violating SLA in three algorithms.
In the experiment, the CAS and PBS algorithms have the same
initial application deployment and the TSD algorithm deploys an
extra MySQL Slaver.
6.3.2. Effects of scaling on response time
We repeat the experiment of Section 6.2 to test the CAS, PBS
and TSD algorithms. The effects of scaling on response time in the
trial of the ‘‘browsing’’ workload are shown in Fig. 15. Each time the
session number is increased or decreased, the PBS algorithm does
not scale the application immediately. The scaling is conducted
after two minutes’ measurement, which means the PBS algorithm
needs a longer scaling time than the other two algorithms. In
addition, the TDS algorithm does not adapt with different types of
workload. This algorithm allocates the same number of servers for
three types of workload and these servers are estimated to handle
the worst-case situation. By contrast, the CAS algorithm identifies
the bottleneck tier and only scales these tiers, thus its scaling time
is the smallest (best).
Fig. 15 shows that the required response time is violated during
the scaling up process. This means a large scaling time would
incur a long period of violating SLA. Fig. 16 lists the proportion
of violating SLA during the whole testing period (5400 s) for
three algorithms. Experiment results show that for each type of
workload, our CAS algorithm can keep the shortest period of
violating SLA.
Result. Our CAS algorithm can perform agile scaling to restore the
response time target, thus ensuring short SLA violation time.
6.3.3. Effects of scaling on cost-efficient services
The cost efficiency of services is measured by the applications’
cost/performance ratio (Definition 11). The performance is repre-
sented by the throughput, which is equivalent to the request ar-
rival rate (see Fig. 11) under the assumption that the request flow
is balanced: the number of requests arriving at the system per unit
time equals the number leaving. Note that the ratio is valid only
when the SLA is met. For example, the ratio is invalid during the
period t = 600–700 s because the scaling up is not completed
until t = 700 s and the response time is violated, as shown in
Fig. 17.
We compare the three algorithms’ effects on cost-efficient
services by first testing the ‘‘browsing’’ workload. The results
in Fig. 17 show that our CAS algorithm can maintain a lower
cost/performance ratio than the other algorithms over most of the
time. The only exception is when the session is decreased, the
CAS algorithm scales down the application earliest and this scaling
down increases the ratio. This is because, whenever the session
number decreases, the throughput drops but the cost remains
unchanged until the scaling down is completed. Hence in the CAS
algorithm, the cost/performance ratio is the largest at the early
stage of each scaling down.
Fig. 17 also shows that the PBS algorithm has the least number
of valid cost/performance ratios because of its longest SLA violation
period. In most cases, the cost/performance ratio is largest (worst)
in the TDS algorithm because it conducts the worst-case capacity
estimation to deploy redundant servers.
We further extend the experiments by testing three types of
workload and list each workload’s average value of cost/perfor-
mance ratio in Fig. 18. Experimental results show that our CAS
algorithm can support applications with the smallest cost/perfor-
mance ratio. This means application owners can spend the least
amount of money for each unit of application performance. Take
the ‘‘browsing’’ workload for example, application owners can
spend 1.54 cents/min to deliver a servicewith a throughput of 100
requests/second using the CAS algorithm. The costs for the same
application performance are 1.72 and1.77 cents/minusing the PBS
and TDS algorithms, respectively.
Result. Using the cost-aware criteria, our CAS algorithm is able to
provide cost-efficient resources for both scaling up and down.
R. Han et al. / Future Generation Computer Systems ( ) – 15
Fig. 17. The cost/performance ratio in the trial of the ‘‘browsing’’ workload using three algorithms.
Fig. 18. The average cost/performance ratios in three algorithms.
7. Discussions
7.1. Discussion on the CAS algorithm
The CAS algorithm presented in this paper is based on reactive
(immediate) scaling of multi-tier applications rather than using
predictive mechanisms. The reactive scaling approaches are used
by most providers, such as Amazon WS [1] and Rightscale [11],
since they are simpler to support and require no prior knowledge
of the workload characteristics. The CAS algorithm uses two
methods to handle the possible errors in capacity estimation. First,
it adds/removes only one server to/from the bottleneck tier in
each estimation analysis and then iteratively conducts the next
estimation. Even if awrong tier is selected in an analysis, thiswould
give prominence to the actual bottleneck tier so that this tier has
a high probability to be selected in the next analysis. Secondly,
the two capacity estimation algorithms are complementary to each
other. For instance, if redundant servers are deployed in the scaling
up, the CAS algorithm can quickly trigger the scaling down to
remedy this problem.
The presented CAS algorithm takes a VM as a basic unit
of resource allocation and scaling. We have investigated other
work [35] on how a similar approach can be used to conduct
the fine-grained application scaling at the resource level itself
(CPUs, memory, I/O) in addition to VM-level scaling. This
approach enables lightweight scaling of applications implemented
using multiple VMs by avoiding heavy-load operations, such as
creation/removal of VMs, and improving resource utilisation of
these VMs as application demands vary. Such a lightweight scaling
approach can be viewed as complementary to the cost-aware
scaling approach proposed in this paper. In addition, the proposed
algorithm and approach fit well for multi-tier applications that
are deployed to provide services for third parties, and where the
variation in the workload changes based on the number of users
using the application, and the type of requests they make. This is
in contrast to other work that is based on scheduling workflow-
based applications [40–44] across multiple resources, and where
the task durations for individual workflow steps can vary based on
the input.
In this paper we have investigated the use of the CAS
algorithm where the cost function was assumed to be the price,
in monetary terms, that the user pays for the servers. The
algorithm can be equally applied with other cost functions, such
as one based on saving energy. Saving energy without sacrificing
application owners’ SLAs has a great economic incentive for cloud
providers. For example, Amazon estimates that power-related
costs occupy 42% of its total budget [45]. The CAS algorithm
provides a good fit for power-aware scaling and can take data
centres’ power consumption into account using the cost function
employed by cloud providers. For example, assume that a server
s is deployed at cloud provider cp with k physical machines
(PMs) {pm1, pm2, . . . , pmk} and its deployment cost is ccp(s). Let
pwr(uj) = (pmax
j − pmin
j )uj + pmin
j be the power consumption of
PM pmj (1 ≤ j ≤ k), where pmax
j is pmj’s power consumption
at the peak load (e.g., 100% utilization), pmin
j is pmj’s minimum
power consumption with the least load (e.g., 1% utilization) and
uj is pmj’s resource utilisation. Hence the power-aware cost
function cpm(cfg(s), pwr(uj)) measures server s’s deployment cost
at pmj, where cfg(s) is s’s VM configuration. According to this
cost function, the starting up or turning off of s’s VM and other
VMs that are running in parallel on pmj influence this PM’s
resource utilisation, thus determining server s’s cost. Taking energy
efficiency as a key indicator in scaling, the cloud provider cp checks
every available PM and identifies the most cost-efficient PM for
server s : ccp(s) = min1≤j≤k{cpm(cfg(s), pwr(uj))}.
16 R. Han et al. / Future Generation Computer Systems ( ) –
7.2. Support for the CAS algorithm across multiple platforms
At present, our iSSe has been implemented and evaluated
as a service component of IC-Cloud [17] to support the CAS
algorithm. We are currently studying extending the iSSe to enable
its integration with other cloud platforms (e.g., Amazon WS). We
are also investigating enabling it to work either as a broker [46]
or as a workflow scheduler [40–44] across multiple providers at
the same time. This is possible since the CAS algorithm itself is
generic and the cost-aware criteria used in the algorithm require
knowledge only of server s’s cost c(s). Such cost is implicitly the
smallest possible cost among the available providers. Suppose
there are n available cloud providers {cp1, cp2, . . . , cpn} providing
similar functionality but have different pricing strategies; i.e.
a server s’s deployment cost is ccpj(s) in cloud provider cpj.
When server s is added, iSSe can automatically choose the cloud
provider that offers the cheapest price: c(s) = min1≤i≤n{ccpi(s)}.
The approach would work well for homogeneous computations.
Addressing heterogeneous computations, e.g. [47,48], is beyond
the scope of our work.
In the current implementation of the CAS algorithm for multi-
tier applications on the IC-Cloud we assume that starting up one
or multiple servers can be achieved within 1 or 2 min and that
it can be completed before the CAS algorithm runs again. The
assumption for the server’s start-up time is realistic on most IaaS
providers, e.g. Amazon WS takes a similar time. This assumption
is also reasonable for the fast varying load of the e-commerce
benchmark used, as a long start-up time would result in an SLA
violation time.
8. Conclusions
In this paper, we have argued that on-demand scaling of
cloud applications raises new challenges for delivering cost-
efficient services. We proposed a cost-sensitive elastic scaling
approach which lowers resource allocation costs by detecting the
bottlenecks in a class of multi-tier applications and accordingly
scales resources upor downonly at these points.Wealso presented
the design and implementation of an intelligent platform based
on our scaling approach to achieve cost-effective elasticity. We
then evaluated and demonstrated the practical effectiveness of our
approach using industry standard benchmarks.
We are currently investigating the development of a more gen-
eral framework for supporting cost-aware adaptive elasticity for a
wider class of applications. Developing such a framework requires
investigating the different dimensions that affect elasticity: includ-
ing the type of computation conducted, its required performance
properties, the user’s budget constraints, the cost models used,
and the general strategy for allocating resources. The application
studied in this paper had certain properties along each of these
dimensions that enabled us to design our proposed approach. In
particular, the computation was based on conducting a fixed set of
transactions serving an end user browsing an e-commerce web-
site. The desired performance requirement was to maintain the
QoS of the application when serving multiple users (measured as
throughput and end-to-end response time for each transaction).
The maximum user budget was not specified explicitly, however
the requirement was minimising users’ cost for meeting the per-
formance target. The cost model assumption was that all VM in-
stances had the same fixed unit cost and that could also be billed at
fine granularity. Moreover, since the application was organised in
multiple tiers, the general strategy usedwas to allocate/de-allocate
resources to/from the tier(s) that had the greatest impact on
performance.
In the general framework under development, we are investi-
gating supporting elasticity for other types of applications, includ-
ing those applications whose computational properties exhibit a
trade-off between the quality of the computational result (QoR)
and computational resources. Examples include data mining ap-
plications where the QoR could be defined by the accuracy of a
predictive model derived from analysing a large data set, with
the possibility of developing more accurate models adaptively by
using more computational resources. Examples also include en-
gineering applications where the QoR, defined by the resolution
of a simulation run, could also be improved adaptively by using
more resources. In both cases the trade-offs between accuracy,
performance and cost are not necessarily trivial to resolve, espe-
cially when using realistic budget constraints and real-time con-
straints. The required decisions become more difficult when using
more comprehensive cost functions, for example those where the
cost of a VM could vary based on its performance properties, and
could also fluctuate over time depending on supply and demand
at single or multiple IaaS providers. Within such a complex set-
ting, the challenge becomes how to develop practical and effective
strategies that help in deciding how, when, and where, to allocate/
de-allocate resources in a cost-effective fashion.
Acknowledgments
The authors would like to thank all other members of the
Discovery Science Research Group, especially Xinyu Liu for her
contribution in developing the system.Wewould also like to thank
Dr Roy Clements and Tania Buckthorp for their helpful comments
on the paper.
References
[1] Amazon Web Services (Amazon WS). http://aws.amazon.com/ (29.10.11).
[2] Microsoft Azure. http://www.microsoft.com/azure/ (29.10.11).
[3] IBM developer cloud. http://www.ibm.com/cloud/developer (29.10.11).
[4] A. Chandra, W. Gong, P. Shenoy, Dynamic resource allocation for shared
data centers using online measurements, in: Proceedings of the 11th
International Conference on Quality of Service, IWQoS’03, Springer-Verlag,
2003, pp. 381–398.
[5] R.P. Doyle, J.S. Chase, O.M. Asad, W. Jin, A.M. Vahdat, Model-based resource
provisioning in a web service utility, in: Proceedings of the 4th Conference on
USENIX Symposium on Internet Technologies and Systems, uSITS’03, USENIX
Association, Berkeley, 2003, pp. 5–5.
[6] B. Urgaonkar, P. Shenoy, Cataclysm: handling extreme overloads in Internet
services, in: Proceedings of the 23rd Annual ACM SIGACT-SIGOPS Symposium
on Principles of Distributed Computing, PODC’04, ACM, St. John’s, Newfound-
land, Canada, 2004.
[7] B. Urgaonkar, G. Pacifici, P. Shenoy, M. Spreitzer, A. Tantawi, An analytical
model for multi-tier Internet services and its applications, in: Proceedings of
the 2005 ACM SIGMETRICS International Conference on Measurement and
Modeling of Computer Systems, SIGMETRICS’05, ACM, Banff, Alberta, Canada,
2005, pp. 291–302.
[8] X. Liu, J. Heo, L. Sha, X. Zhu, Adaptive control of multi-tiered web
applications using queueing predictor, in: 10th IEEE/IFIP Network Operations
and Management Symposium, NOMS 2006, IEEE, Vancouver, BC, 2006,
pp. 106–114.
[9] B. Urgaonkar, P. Shenoy, A. Chandra, P. Goyal, Dynamic provisioning of multi-
tier Internet applications, in: Second International Conference on Autonomic
Computing, ICAC’05, Seattle, Washington, 2005, pp. 217–228.
[10] B. Urgaonkar, P. Shenoy, A. Chandra, P. Goyal, T. Wood, Agile dynamic
provisioning of multi-tier Internet applications, ACM Transactions on
Autonomous and Adaptive Systems 3 (2008) 1–25.
[11] Rightscale. http://www.rightscale.com/ (29.10.11).
[12] Unicloud. http://www.univa.com/products/unicloud (29.10.11).
[13] A. Nathani, S. Chaudhary, G. Somani, Policy based resource allocation in IaaS
cloud, Future Generation Computer Systems 28 (2011) 94–103.
[14] D.A. Bacigalupo, J. van Hemert, X. Chen, A. Usmani, A.P. Chester, D.N.
Dillenberger, G.B. Wills, L. Gilbert, S.A. Jarvis, Managing dynamic enterprise
and urgent workloads on clouds using layered queuing and historical
performance models, Simulation Modelling Practice and Theory 19 (2011)
1479–1495.
[15] J. Bi, Z. Zhu, R. Tian, Q. Wang, Dynamic provisioning modeling for virtualized
multi-tier applications in cloud data center, in: 2010 IEEE 3rd International
Conference on Cloud Computing, CLOUD’10, IEEE, Miami, Florida, 2010,
pp. 370–377.
[16] Y. Hu, J. Wong, G. Iszlai, M. Litoiu, Resource provisioning for cloud computing,
in: Proceedings of the 2009 Conference of the Center for Advanced Studies on
Collaborative Research, CASCON’09, ACM, 2009, pp. 101–111.
R. Han et al. / Future Generation Computer Systems ( ) – 17
[17] L. Guo, Y. Guo, X. Tian, IC Cloud: a design space for composable cloud
computing, in: 2010 IEEE 3rd International Conference on Cloud Computing,
CLOUD’10, IEEE, Miami, Florida, 2010, pp. 394–401.
[18] TPC Transaction Processing Performance Council. http://www.tpc.
org/ (29.10.11).
[19] R. Buyya, C.S. Yeo, S. Venugopal, J. Broberg, I. Brandic, Cloud computing and
emerging IT platforms: vision, hype, and reality for delivering computing as
the 5th utility, Future Generation Computer Systems 25 (2009) 599–616.
[20] R. Buyya, R. Ranjan, R.N. Calheiros, Modeling and simulation of scalable
cloud computing environments and the cloudsim toolkit: challenges and
opportunities, in: International Conference on High Performance Computing
& Simulation, HPCS’09, IEEE, Leipzig, 2009, pp. 1–11.
[21] L. Rodero-Merino, E. Caron, A. Muresan, F. Desprez, Using clouds to scale
grid resources: an economic model, Future Generation Computer Systems 28
(2011) 633–648.
[22] H.C. Lim, S. Babu, J.S. Chase, S.S. Parekh, Automated control in cloud
computing: challenges and opportunities, in: Proceedings of the 1stWorkshop
on Automated Control for Datacenters and Clouds, ACDC’09, ACM, Barcelona,
Spain, 2009, pp. 13–18.
[23] P. Martin, A. Brown, W. Powley, J.L. Vazquez-Poletti, Autonomic management
of elastic services in the cloud, in: IEEE Symposium on Computers and
Communications, ISCC’11, IEEE, Kerkyra, 2011, pp. 135–140.
[24] E. Casalicchio, L. Silvestri, Architectures for autonomic service management
in cloud-based systems, in: IEEE Symposium on Computers and Communica-
tions, iSCC’11, IEEE, Kerkyra, 2011, pp. 161–166.
[25] A.J. Ferrer, F. Hernández, J. Tordsson, E. Elmroth, A. Ali-Eldin, C. Zsigri, R.
Sirvent, J. Guitart, R.M. Badia, K. Djemame, OPTIMIS: a holistic approach to
cloud service provisioning, Future Generation Computer Systems 28 (2011)
66–77.
[26] J.O. Kephart, D.M. Chess, The vision of autonomic computing, Computer 36
(2003) 41–50.
[27] K. Xiong, H. Perros, Service performance and analysis in cloud computing,
in: 2009 Congress on Services-I, IEEE, Los Angeles, CA, 2009, pp. 693–700.
[28] R. Ghosh, K.S. Trivedi, V.K. Naik, D.S. Kim, End-to-end performability
analysis for infrastructure-as-a-service cloud: an interacting stochastic
models approach, in: 2010 IEEE 16th Pacific Rim International Symposium on
Dependable Computing, IEEE, Tokyo, Japan, 2010, pp. 125–132.
[29] R. Ghosh, F. Longo, V.K. Naik, K.S. Trivedi, Quantifying resiliency of IaaS cloud,
in: 2010 29th IEEE Symposium on Reliable Distributed Systems, IEEE, New
Delhi, Punjab India, 2010, pp. 343–347.
[30] R. Pal, P. Hui, On the economics of cloud markets, Technical Report, University
of Southern California, Los Angeles, 2011, pp. 1–7.
[31] T. TruongHuu, G. Koslovski, F. Anhalt, J. Montagnat, P. Vicat-Blanc Primet, Joint
elastic cloud and virtual network framework for application performance-cost
optimization, Journal of Grid Computing (2011) 1–21.
[32] W. Iqbal, M.N. Dailey, D. Carrera, P. Janecek, Adaptive resource provisioning
for read intensive multi-tier applications in the cloud, Future Generation
Computer Systems 27 (2011) 871–879.
[33] J.Z. Li, Fast optimization for scalable application deployments in large
service centers, Department of Systems and Computer Engineering, Carleton
University, Ottawa, Ontario, 2011, p. 139.
[34] VMware OVF. http://www.vmware.com/appliances/getting-started/learn/
ovf.html (29.10.11).
[35] R. Han, L. Guo, M.M. Ghanem, Y. Guo, Lightweight resource scaling for
cloud applications, in: The 12th IEEE/ACM International Symposium on
Cluster, Cloud and Grid Computing, CCGrid’12, IEEE, Ottawa, Canada, 2012,
pp. 644–651.
[36] R.N. Calheiros, R. Ranjan, C.A.F. De Rose, R. Buyya, Cloudsim: a novel
framework for modeling and simulation of cloud computing infrastruc-
tures and services, Technical Report, Grid Computing and Distributed Sys-
tems Laboratory, The University of Melbourne, Melbourne, 2009, pp. 1–9.
http://arxiv.org/abs/0903.2525.
[37] R.B. Cooper, Introduction to Queueing Theory, second ed., North-Holland, New
York, 1981.
[38] R. Jain, The Art of Computer Systems Performance Analysis, JohnWiley & Sons,
New York, 1991.
[39] R. Han, L. Guo, Y. Guo, S. He, A Deployment platform for dynamically scaling
applications in the cloud, in: 3rd IEEE International Conference on Cloud
Computing Technology and Science, CloudCom’11, IEEE, Athens, Greece, 2011,
pp. 506–510.
[40] C.C. Hsu, K.C. Huang, F.J. Wang, Online scheduling of workflow applications in
grid environments, Future Generation Computer Systems 27 (2011) 860–870.
[41] R. Han, Y. Liu, L. Wen, J. Wang, Probability timing constraint WF-nets and
their application to timing schedulability analysis of workflow management
systems, in: 2009 WRI World Congress on Computer Science and Information
Engineering, CSIE 2009, IEEE Computer Society, Los Angeles, CA, 2009,
pp. 669–673.
[42] R. Han, Y. Liu, L. Wen, J. Wang, Dynamically analyzing time constraints in
workflow systems with fixed-date constraint, in: Proceedings of the Twelfth
Asia-Pacific Web Conference, APWeb 2010, IEEE Computer Society, Busan,
Korea, 2010, pp. 99–105.
[43] M. Ghanem, Y. Guo, A. Rowe, P. Wendel, Grid-based knowledge discovery
services for high throughput informatics, in: Proceedings of the Eleventh IEEE
International Symposium on High Performance Distributed Computing, IEEE,
Edinburgh, Scotland, 2002, p. 416.
[44] S. AlSairafi, F.S. Emmanouil, M. Ghanem, N. Giannadakis, Y. Guo, D.
Kalaitzopoulos, M. Osmond, A. Rowe, J. Syed, P. Wendel, The design
of discovery net: towards open grid services for knowledge discovery,
International Journal of High Performance Computing Applications 17 (2003)
297–315.
[45] J. Hamilton, Cooperative expendablemicro-slice servers (CEMS): low cost, low
power servers for Internet-scale services, White Paper, 2009.
[46] J. Tordsson, R.S. Montero, R. Moreno-Vozmediano, I.M. Llorente, Cloud
brokering mechanisms for optimized placement of virtual machines across
multiple providers, Future Generation Computer Systems 28 (2012) 358–367.
[47] N. Azam, M. Ghanem, D. Kalaitzopoulos, A. Wolf, V. Kasam, Y. Wang, M.
Hofmann-Apitius, DockFlow: achieving interoperability of protein docking
tools across heterogeneous grid middleware, International Journal of Ad Hoc
and Ubiquitous Computing 6 (2010) 235–251.
[48] J. Darlington, M. Ghanem, Y. Guo, H.W. To, Guided resource organisation in
heterogeneous parallel computing, International Journal of High Performance
Computing 4 (1997) 13–23.
Rui Han is a researcher and Ph.D. student at the
Department of Computing, Imperial College London, UK.
He received M.Sc. from Tsinghua University, China. His
research interests are cloud computing, cloud resource
management andworkflow technology. He is experienced
in the design and development of cloud deployment
platforms and process-aware information systems.
Moustafa M. Ghanem is a research fellow at the
Department of Computing, Imperial College London, UK.
He holds a Ph.D. and an M.Sc. in high performance
computing from Imperial College London. He is a Research
Fellow in the Department of Computing, Imperial College
London. His current research interests are in large-scale
informatics applications, including large-scale data and
text mining applications and infrastructures, Grid and
Cloud computing and workflow systems for e-Science
applications. He has published more than 80 papers in
these areas. Moustafa Ghanem was previously Research
Director at InforSense Ltd and VP for Research at Nile University, Egypt.
Li Guo is a post-doc researcher and research associate
in computing science at the Department of Computing,
Imperial College London, UK. He received Ph.D. degree in
artificial intelligence at the University of Edinburgh, UK.
He has been working in the area of grid computing and
cloud computing since 2006. For the past 5 years, he has
been involved in major grid and cloud related EU and UK
projects. He is the chief architect of the Imperial College
Cloud platform. His research interests include large scale
distributed systems, intelligent applications, and cloud
computing.
Yike Guo is a professor in computing science at the De-
partment of Computing, Imperial College London, UK. He
graduated in computer science from Tsinghua University,
PRC and received a Ph.D. degree in computational logic and
declarative programming at Imperial College London. He
has been working in the area of data intensive analytical
computing since 1995, when hewas the technical director
of Imperial College Parallel Computing Centre. During the
past 10 years, he has been leading the data mining group
of the department to carry out many research projects,
including some major UK e-science projects such as a
discovery net on grid based data analysis for scientific discovery,MESSAGE onwire-
less mobile sensor network for environment monitoring, Biological Atlas of Insulin
Resistance (BAIR) on system biology for diabetes study. He has focused on apply-
ing data mining technology to scientific data analysis in the fields of life science
and healthcare, environment science and security. His research interests include
large-scale scientific data analysis, data mining algorithms and applications, paral-
lel algorithms, and cloud computing.
Michelle Osmond is a post-doc researcher and research
associate in computing science at the Department of
Computing, Imperial College London, UK. She has a Ph.D.
and M.Sc. in Computing Science from Imperial College
London and a B.Sc. in Physics. Her research interests
include cloud computing, data mining and bioinformatics.
View publication stats
</pre>
                </body>
            </html>